### Введение

    Мне всегда было интересно можно ли собирать домашнии компьютеры не только для игр или для работы, а для чего-то еще ?
    Этим что-то еще стало AI и получится ли использовать этот комп
     для AI(Artificial intelligence) в частности для ML.
    Я постапвил перед собой задачу выяснить это.
    Постораюсь использовать  несколько компьютеров собранных в разное время и результаты которые они выдадут.
    Во всех вариантах буду запускать скрипт на Python, который должнет выдовать результаты для разного количества входных данных:
    samples = [50_000, 100_000, 250_000, 500_000, 750_000, 1_000_000]
    и строить, на основнии этого график, отдельно для CPU и GPU.
    В дальнейшем надеюсь сравнить результаты разных копьютеров и на сколько процентов произошел прирост в производительности.
    Конечно возможны ошибки и все это относительно, но все равно, все это очень интересно :)
    Результат работы скрипта: Artificial intelligence.ipynb сохраню в виде csv-файла и поcтроим график.

* FirstPC:
  * Intel Core i7-2600CPU - 2011 года - подойдет(встроенное графическое ядро HD Graphics 2000 с максимальной частотой 1350 МГц)
  * Nvidia GeForce GTX 750 Ti - 4GB - 2014 года - для обучения, не подойдет
  * DDR3 - 24GB
* SecondPC:
  * AMD Ryzen5 3600 - 2018 года - подойдет
  * ASUS DUAL RTX 3050 6 GB - 2022 года - подойдет
  * DDR4 - 32GB

```txt
| Устройство       | Поддержка GPU в XGBoost  | Практическая польза |
|------------------|--------------------------|----------------------|
| GTX 750 Ti       | ❌ нет (устарела)        | Замедление           |
| RTX 3050         | ✅ да                    | Ускорение в 2–10×     |
| CPU (i7-2600)    | ✅                       | Может быть быстрее на малых выборках |

```

```text
Какими минимальными характеристиками должен обладать 
домашний компьютер 
для реализации Машинного обучения с учителем 
с использованием типа регрессии, 
для прогнозирования некоторых значений, полученных в результате эксперимента?
```

Результат:

1. Минимальные аппаратные характеристики:
    * 1.1 Процессор (CPU)
    Минимум: 4-ядерный, например, Intel Core i5 или AMD Ryzen 5 (модели начиная с 2017 года).

        Рекомендация: Тактовая частота ≥ 2.5 ГГц, поддержка AVX (для ускорения NumPy и Scikit-learn).

        Зачем: Обработка данных (особенно при подготовке и обучении моделей), если нет GPU — модели обучаются на CPU.
    * 1.2 Оперативная память (RAM)
        Минимум: 8 ГБ.

        Рекомендация: 16 ГБ — если работать с большими датафреймами (более 1 млн строк).

        Зачем: Хранение набора данных, промежуточных результатов, моделей.
    * 1.3 Графический процессор (GPU)
        Минимум: не требуется, если не используешь нейросети или большие тензоры (например, TensorFlow или PyTorch).

        Рекомендация (если использовать нейросети):

        NVIDIA с поддержкой CUDA (например, GTX 1660, RTX 2060+)

        VRAM: ≥ 4 ГБ.

        Зачем: Ускорение обучения нейросетей; для классической регрессии (линейной, дерева решений и т.д.) — не нужен.

2. Оценка и расчёт требований (приблизительно):
    * 2.1 Формула оценки объёма RAM:
        * RAM_needed ≈ size_of_dataset_in_memory × 3

    Пример: если CSV весит 500 МБ, то в pandas он займёт ~1.5 ГБ RAM.

    Нужно минимум втрое больше RAM, чем размер вашего DataFrame, чтобы комфортно обучать модель.

    * Формула для CPU нагрузки:
        * Training_time ∝ dataset_size × model_complexity / processing_power - чем меньше результат при больших входных данных, тем быстрее обучение.Training_time - это не само время в сек, а некоторое значение пропорциональное сек: Training_time_per_sec (в секундах) ≈ K × Training_time.
        Где:
        * dataset_size — количество примеров в обучающем наборе (в мегабайтах, строках или признаках),

        * model_complexity — условная метрика, зависящая от количества признаков, глубины модели, регуляризации и т.п.,

        * processing_power ≈ CPU_cores × frequency (GHz) × IPC.
            * (instructions per cycle — зависят от архитектуры).

    ```txt
    Допустим у нас:
        * 500 000 строк (объектов),
        * 20 признаков на строку,
        → Всего:
            500,000 × 20 = 10,000,000 чисел

    Размер одного числа
    Обычно данные для обучения представлены как:
    float64 (в numpy, pandas по умолчанию): 8 байт на число,
    или float32: 4 байта (меньше, но менее точное).
    float64:
    10,000,000 чисел × 8 байт = 80,000,000 байт = 80 MB(76.2939)
    float32:
    10,000,000 × 4 байта = 40,000,000 байт = 40 MB(38.147)

    ```

    ```txt
     Пример 1: Простая линейная регрессия на ноутбуке:
        Условия:
            * Dataset: 100 000 строк × 10 признаков (примерно 8 MB).
            * Модель: обычная линейная регрессия (model_complexity ≈ 1).
            * Процессор: 4 ядра × 2.5 GHz × IPC ≈ 1.2 (ипично для старых ноутбуков).
            * Обработка чисел — только на CPU.

            processing_power = 4 × 2.5 × 1.2 = 12
            Training_time ∝ 100 000 × 10 / 12 ≈ 83 333 условных единиц
    
    ```

    ```txt
    Пример 2: Градиентный бустинг на современном ПК:
        Условия:
            * Dataset: 1 000 000 строк × 50 признаков (примерно 400 MB).
            * Модель: XGBoost 
            (model_complexity ≈ 10–20 в условных единицах).
            * Процессор: 8 ядер × 3.6 GHz × IPC ≈ 1.5.
            * Поддержка многопоточности.

            processing_power = 8 × 3.6 × 1.5 = 43.2
            Training_time ∝ 1 000 000 × 50 × 15 / 43.2 ≈ 17 361 111 условных единиц
    ```

3. Пример минимальной конфигурации:

```txt
* Компонент   |            Минимум          |        Рекомендуется
* CPU         |Intel i5-7500 / Ryzen 5 1600 |   Intel i5-12400 / Ryzen 5 5600
* RAM         |      8 ГБ                   |           16 ГБ
* GPU         |  — (необязательно)          |NVIDIA GTX 1660 / RTX 3060 (если нейросети)


```
